{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/gist/ds-semenov/54360e66c37d33eaf0a4bbabc08795a4/sib200_ru_bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5TPy3bG7IHRZ",
    "outputId": "3b96513d-dd86-421a-db56-43ae8fcb2f8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/404.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install datasets transformers evaluate optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iIXBt1ztIMQg"
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AB0SF2I9Imhe"
   },
   "outputs": [],
   "source": [
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "J42Xa9sxINXI"
   },
   "outputs": [],
   "source": [
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tPVrxoGZIP_P"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, concatenate_datasets\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "import optuna\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import TrainingArguments, Trainer, set_seed\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "c6f99b3c1a544b7c8f5ea8d3f3b178ec",
      "a2dc47fe215f45e3ab491e8dea1a46bb",
      "ee464bb05c324c70a30d4af84233f1ce",
      "e383ebdc61f247b4882557683160d28c",
      "6ee2bcab75c343cab42004021a254923",
      "d5432d740cde44ef9753735ae74061cf",
      "baccc18a230f4a4899b1c7e225d819be",
      "ac0b03af54fb479c8fc777723c8f345b",
      "8bb60f45ff524853941a5577af8f98a3",
      "e978a5a78ac74d1b96ab744e3953e5b1",
      "fd6f9fc4aff44aed94ec5eee09cd6d27",
      "a402961762134e0f846be0c5905c5e35",
      "fc008629ba4c4f6789911ec052180f6d",
      "e9b1a326e1804749b7da6c630dbde003",
      "14a9e992fdcd4a53b00e58a7c1ea23b0",
      "dcd3ac7966ad46c1b77caf04de204af2",
      "09318254ebcd4b7681fbe2391843daf5",
      "a69df4f46dfa40648e879085f8da2afd",
      "2bdee722c12746908a593a789343e59f",
      "9b197208b4794ad2ba1265863687c108",
      "9841efbedb624d5d81d5f48748a0730a",
      "a7736fdf10e944118e15b4ac98e9e72d",
      "f785f59c39744ce5bc698c055cd5b926",
      "bfe3f127ae2241e4bf22e1e2396b8dfd",
      "58b8d38b109245daa6a3dfbbed23c7c7",
      "2f42219d98034c8d973bd1cb0696e6a7",
      "122237e88f644db8aa23b471908d55ef",
      "ee9d0ee916a844da9cc7b0136336306a",
      "d2391420f637426383f76cdee0034c30",
      "9c24eb73df7c41948309ad828ef82a0f",
      "c0957ababa6a4b2cb5b9556dc62b6442",
      "0cdf9eb7b04c4f7cbd1738df6dd16836",
      "a05ff9088e8c42f19eecd170de3e3a2f"
     ]
    },
    "collapsed": true,
    "id": "zUHP8cf7JIOm",
    "outputId": "a7b7f428-b9de-4d45-a09f-96090e069681"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "config.json:   0%|          | 0.00/674 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MODEL_NAME = \"ai-forever/ruRoberta-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241,
     "referenced_widgets": [
      "89488db01f844d6c8a801636d3ee2bbc",
      "8d09dca842be4b2a9db26fa404103bcb",
      "772e619885e5455e9aef6d655016503a",
      "4f26bad9f9824669bc0abdf4e3b69e66",
      "f520eb95628843eaab82e3efd131f52e",
      "87c44384242040b690ed302725fdc839",
      "501ad222eec845239d08948145fc8e31",
      "2b03209d9347403db57bb755b61bace3",
      "46729395705a467d97fe4c3f94c15da9",
      "22b9ca6afb1b4e65993f3cc02955817a",
      "0478f770ec6549b587f21a26398e1e7b",
      "7cf4b0354eb6499aa0b1ee7e6a8f0e38",
      "a15fad8f3aa341fc9e471e6039a8fd69",
      "159e3a5194cd495d83ae6b86d47fb2e8",
      "df074ddd56084df6aa708616c94ec574",
      "44c61ec576234fd3a14c572e9466304c",
      "c5a6f66f50004c3b854e3190845dccbf",
      "8fe35de7b3444f8c9f2dbed79670a3c3",
      "6cbb24fce41840daa193a328b1f901a4",
      "737ba70a58754487b1558ef67d1dd1d0",
      "79651350f2b943819183d298bc18005b",
      "f90fedf36bec4c69a4a55e4e449d162d",
      "3e3d7b2d305a4d8494d2958603b40336",
      "483d8629a60d4ec5a68618fc91187db5",
      "7d134b9660ed42939e799a713211a806",
      "2163eb78182445af8db020e497d6b889",
      "4ac59df1a2774d4b92becc77cc75d367",
      "1d9221d2cae54425a4a4409680bc3741",
      "3f05d05894044f8f8fd26ced82098674",
      "d696695732cc4d70ac859e919addb5da",
      "371f0ba5fbb847ebab24aeaa51046546",
      "ba544fa241174c2eaa1b77a7f8424051",
      "eb00650ef2324742b2b0e7ea550049ae",
      "2f206342efa944639b5fabaf92df956d",
      "f4ff270105c34c14af66f75227ee54c9",
      "67904a5348d54d808018f5d251b2659d",
      "fd2f8819eb4b4cd889aa6f6d974706b9",
      "955b5c600fc347ccba37b34c3397339d",
      "6fc6d4fab1c24edf9ce79b7bf640c17f",
      "ca5b3ec9f6b6436cb3a89a2ed153c69d",
      "fd2d60b8af0746a699cd8032cc9f9e6a",
      "e357ffa23cab43a29a4839527303e5d3",
      "425534181e264859b5421acdd9d6867d",
      "b40457291a654de480462827b0905bcb",
      "8e79e67a34ca4225b1f4d013dcc891e3",
      "4f66328dd4e6440c96a1328da12c389e",
      "f414c7c1538348089415e9d2cf849ba7",
      "9bcf3164923141bc8bb55086ea1a2fe9",
      "0f7dc05c3d9e446c80da5f464cfcba33",
      "36fbadfb35504de6bd18e360c3a8a297",
      "b690be1f6f4e4591a5e606a448a835b3",
      "c6ebdfeb6a154ea9892e51a67a802a2c",
      "e4a997c1592f494f92e1834c7b030167",
      "e179524821224574851ceb82e60501c9",
      "f7482c1ee0ce4ee68e90d75661f0f5d8",
      "8531910bb58e4bfeadab875b8d08be80",
      "88dbde03097d47efb93ba15c0fcc8bee",
      "f6fba5ef30814ad3aa002c716b9ac524",
      "472e8e5cad454c56bd3cf338353d4b5f",
      "822a21a3fa144a1f8c9e44c8a8d3ec94",
      "4b1bd2156f274e3890f9211e0b81d86b",
      "24f51b173e004e819e16b7efec07a307",
      "f08b4158383242dea10b6fa61359d92c",
      "b1a6a604d1314841a19073e8ccb49609",
      "638e260f1a1946eaa2ecd1d09a0f028f",
      "76336ad290c94c6ab52b074a533dbb36",
      "1cac80db86a84f618e8348f54aee6556",
      "a5c7f648c55c4634b484f91b46614861",
      "944d0ee606cc4ccd8a7f44b2ca5defd7",
      "844f0125737e43f79279b6724e5f9f99",
      "5842c1644a124ffab25caf641ca24070",
      "7c0c3e761652460ea731db5005a7c923",
      "8380835b84e64bab9a4934070badb060",
      "d32d241930044419951aec6123748c02",
      "e6b09a082b8a4d0189fec9025c041cb0",
      "d2010250db35436fa98ecb4caec89e8b",
      "140ea0e4d0804675be18bf28a23ae44f"
     ]
    },
    "collapsed": true,
    "id": "yk7rg3ZNKWaM",
    "outputId": "772e77e3-3a81-4c10-c29f-0b67e3c123af"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "train.tsv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "dev.tsv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "test.tsv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "DATASET_NAME = 'Davlan/sib200'\n",
    "DATASET_LANGUAGE = 'rus_Cyrl'\n",
    "train_set = load_dataset(DATASET_NAME, DATASET_LANGUAGE, split='train')\n",
    "validation_set = load_dataset(DATASET_NAME, DATASET_LANGUAGE, split='validation')\n",
    "test_set = load_dataset(DATASET_NAME, DATASET_LANGUAGE, split='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Vu0zedSWx9C",
    "outputId": "58c96ebf-2c5f-4e55-f550-83188c80b04c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['index_id', 'category', 'text'],\n",
      "    num_rows: 701\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "d192b1730378415ba93b8ab833321ea0",
      "4cce081ed1a14b62a9e5ec790b3d4484",
      "26059308d6934a82834b476d04384027",
      "dc2f38bb3ae542ad83d7215b38e44328",
      "345876684b9f4163af730813f8911353",
      "ca32ae6858014e4898bccbe22914f621",
      "d3b9e1861ebf42e29f51fa0f2c2c480c",
      "c938cffc3e254b20a97f18df5857f1d6",
      "a096a9f549654a30b9a9597286f39241",
      "eda32575f60346cea6e4f6804263375d",
      "5180ae5f8baa430794bf27d846bc5f1a",
      "984b543469534556ac0f689d171c47ef",
      "a9f056455c4f43519edcadf57c80c93f",
      "c7572bbaf4e4480b818b2caa528fe3ae",
      "6185ae12b1cc4876a21afd393890f6f2",
      "e13b50303d93457190ea3b36ba54e1e0",
      "62d56d94e787412bab7e9ed7be670fbb",
      "4574c2abee7548c4a240da8e6a64ce79",
      "974fcf2551c44ed989cb215cb25495ff",
      "aa799ff118d043849c77b155e3511f73",
      "43e4e0b932e649ffa1ccb22cdd305d8c",
      "b5b3f3dfb2b247a1b6c0137ba87447b6",
      "cf9c473ba17547718493a1e38f6f1cd8",
      "c3b6af48a94941cfb0191781d04ab522",
      "3cc769b78ad0416aae0b7bb3588f7026",
      "97cf9a99758649008c40a6202c818469",
      "a2fff31c5bf94b7bb76018542163b8ed",
      "5113739121f4419e9fe542a080701b13",
      "60a32224c19f4c3eb9da053d80d442df",
      "e935913cfb404490b7098d698efe88e9",
      "9807aec9bebe4e5da405558539fd9b3c",
      "5a24b84d98a1426eb0be7a0c52391d1e",
      "d2fd01cc3985455c8198f26859f25cef"
     ]
    },
    "id": "VhaWEbS4K2GE",
    "outputId": "4e5ea64e-f015-4571-b281-43e60f1f68ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/701 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/99 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "MAX_LEN = 192\n",
    "MINIBATCH_SIZE = 64\n",
    "\n",
    "tokenized_train_set = train_set.map(\n",
    "    lambda it: tokenizer(it[\"text\"], truncation=True, max_length=MAX_LEN),\n",
    "    batched=True, batch_size=MINIBATCH_SIZE, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_validation_set = validation_set.map(\n",
    "    lambda it: tokenizer(it[\"text\"], truncation=True, max_length=MAX_LEN),\n",
    "    batched=True, batch_size=MINIBATCH_SIZE, remove_columns=[\"text\"]\n",
    ")\n",
    "tokenized_test_set = test_set.map(\n",
    "    lambda it: tokenizer(it[\"text\"], truncation=True, max_length=MAX_LEN),\n",
    "    batched=True, batch_size=MINIBATCH_SIZE, remove_columns=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OWcOQFrFWsS8",
    "outputId": "25db08a4-b508-476f-f0c2-697e09104d28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['index_id', 'category', 'input_ids', 'attention_mask'],\n",
      "    num_rows: 701\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "bb5d529147624d27bf9716884efcf68f",
      "926faab3f6dd40b39a542e677b46ddaa",
      "fd65c7799fac4f7eb06903ac06d98663",
      "a38102735a094ad99b8b49a197d7f3ee",
      "6f501bb8435b429b9cddb3b251d05516",
      "d15fd4185f224b69b123bf9824a1a95f",
      "384dabd4ac71412b8accadb54425f17a",
      "2f52cf2636f14e308e617f843b0fb26a",
      "af71e7c85580499f92dacbfa944cd7e6",
      "6ba24038e1b04be1a071027d73adeb89",
      "babecc0cc9104faa8fdea291aaff1579"
     ]
    },
    "id": "lyKhLV1_Jept",
    "outputId": "4896ea8b-601e-4f7c-9bc1-c7dfd1784423"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cls_metric = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "2uDT6RjVL1wf"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return cls_metric.compute(predictions=predictions, references=labels, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "E9Rjdi7RLxv-"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rsnTjVcyLyco",
    "outputId": "f56a2e47-0bf8-46f8-c3c6-e672319c9cf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categories for classification are: ['entertainment', 'geography', 'health', 'politics', 'science/technology', 'sports', 'travel']\n"
     ]
    }
   ],
   "source": [
    "list_of_categories = sorted(list(\n",
    "    set(train_set['category']) | set(validation_set['category']) | set(test_set['category'])\n",
    "))\n",
    "indices_of_categories = list(range(len(list_of_categories)))\n",
    "n_categories = len(list_of_categories)\n",
    "print(f'Categories for classification are: {list_of_categories}')\n",
    "id2label = dict(zip(indices_of_categories, list_of_categories))\n",
    "label2id = dict(zip(list_of_categories, indices_of_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "M5MH8Lluk6lx"
   },
   "outputs": [],
   "source": [
    "labeled_train_set = tokenized_train_set.add_column(\n",
    "    \"label\",\n",
    "    [label2id[val] for val in tokenized_train_set[\"category\"]]\n",
    ").remove_columns([\"category\"])\n",
    "\n",
    "labeled_validation_set = tokenized_validation_set.add_column(\n",
    "    \"label\",\n",
    "    [label2id[val] for val in tokenized_validation_set[\"category\"]]\n",
    ").remove_columns([\"category\"])\n",
    "\n",
    "labeled_test_set = tokenized_test_set.add_column(\n",
    "    \"label\",\n",
    "    [label2id[val] for val in tokenized_test_set[\"category\"]]\n",
    ").remove_columns([\"category\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kXHuKR5Rp5ah",
    "outputId": "aa5d0a04-f86b-4ab2-dcf9-1e6299fb716f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['index_id', 'input_ids', 'attention_mask', 'label'],\n",
      "    num_rows: 701\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(labeled_train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "83dcbfd7e85445dfb4d6ed0648024fe2",
      "dc76c58ba074441b9239043892b5a9e4",
      "2722d783d8a540adba75d29ce7b97e94",
      "0b255fb7971a4cd183bc65207dfbf1af",
      "0ca0364c27c847a3afcaec13ad01fedd",
      "bd1018382e3e4cfca4b78dc64d24ffb6",
      "e6a349bace204a46a85fb1f66fa87850",
      "cd482430389d456e954e6d0de0bba0c2",
      "e03a6d983d7848efa9c274c31957059b",
      "0a5ac0de0a93460db1aa28208d25fac0",
      "feffb044123d41cdad2e3dce67976dbf",
      "e9ee8635ccf6464abcf77d3deff4d8ef",
      "0f914443bae34275bd8e3408f33549b3",
      "cb8e2baa47b345099e8aa237518bbab3",
      "13ded3dcc5be43b0bd08d54b0715ed04",
      "5eb85f54f48f41179056d10ce4f689bd",
      "2729c4ea2c374d1597ea8783e9b342b0",
      "e241f0f5066f45228a0f50a799a7d250",
      "4f8a70428e934e1b90a58e9d3dd8b1d5",
      "b207dc47556b4eaf9d2d5e24352f5d85",
      "863b075043314533acab3942c34c45bb",
      "9b9b7d44a3d2442dbbd4ac5ba3f40752"
     ]
    },
    "id": "eMLvQYsJNhrq",
    "outputId": "26a48245-4284-43e9-8147-42e0da03a26f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "[I 2025-12-24 05:00:05,309] A new study created in memory with name: no-name-8e31901b-1469-46ab-ae22-cfdea1f79457\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 04:22, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.890600</td>\n",
       "      <td>1.627137</td>\n",
       "      <td>0.497184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.230500</td>\n",
       "      <td>0.909500</td>\n",
       "      <td>0.780722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.765800</td>\n",
       "      <td>0.809415</td>\n",
       "      <td>0.793152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.640600</td>\n",
       "      <td>0.802673</td>\n",
       "      <td>0.820985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:04:47,929] Trial 0 finished with value: 0.8209848006399731 and parameters: {'learning_rate': 8.353511893335669e-06, 'weight_decay': 0.0020551350738586315, 'warmup_ratio': 0.0820948318868611, 'label_smoothing_factor': 0.08024558466615211, 'num_train_epochs': 4, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 8, 'hidden_dropout_prob': 0.1681115315063974, 'attention_probs_dropout_prob': 0.23712756186445133, 'classifier_dropout': 0.033282504102332286}. Best is trial 0 with value: 0.8209848006399731.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 04:18, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.772000</td>\n",
       "      <td>1.341875</td>\n",
       "      <td>0.686496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.941700</td>\n",
       "      <td>0.881091</td>\n",
       "      <td>0.812566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.641600</td>\n",
       "      <td>0.895271</td>\n",
       "      <td>0.802252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.577800</td>\n",
       "      <td>0.916179</td>\n",
       "      <td>0.801983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.560200</td>\n",
       "      <td>0.922435</td>\n",
       "      <td>0.820985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.525900</td>\n",
       "      <td>0.930313</td>\n",
       "      <td>0.815089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:09:11,670] Trial 1 finished with value: 0.815088859846073 and parameters: {'learning_rate': 1.6977593352790607e-05, 'weight_decay': 0.05577130801688837, 'warmup_ratio': 0.04263205223237627, 'label_smoothing_factor': 0.10285088459819729, 'num_train_epochs': 6, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 16, 'hidden_dropout_prob': 0.07211449592342574, 'attention_probs_dropout_prob': 0.05297419299196853, 'classifier_dropout': 0.2619604308971315}. Best is trial 0 with value: 0.8209848006399731.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 01:14, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.988000</td>\n",
       "      <td>2.012863</td>\n",
       "      <td>0.052812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.954900</td>\n",
       "      <td>1.981604</td>\n",
       "      <td>0.110201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.951300</td>\n",
       "      <td>1.972622</td>\n",
       "      <td>0.115770</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:10:30,103] Trial 2 finished with value: 0.1157699443413729 and parameters: {'learning_rate': 1.4705220726064946e-06, 'weight_decay': 0.0486363128584637, 'warmup_ratio': 0.03198495034318338, 'label_smoothing_factor': 0.10825648633129582, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 4, 'hidden_dropout_prob': 0.13471077369428558, 'attention_probs_dropout_prob': 0.07690662951430415, 'classifier_dropout': 0.17737376468833732}. Best is trial 0 with value: 0.8209848006399731.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 00:48, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.958500</td>\n",
       "      <td>1.984039</td>\n",
       "      <td>0.049190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.911500</td>\n",
       "      <td>1.956862</td>\n",
       "      <td>0.105779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.892800</td>\n",
       "      <td>1.946166</td>\n",
       "      <td>0.164568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.867600</td>\n",
       "      <td>1.938011</td>\n",
       "      <td>0.146068</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:11:22,169] Trial 3 finished with value: 0.1460675886176257 and parameters: {'learning_rate': 3.4783475910894596e-06, 'weight_decay': 0.02007261642088826, 'warmup_ratio': 0.10440294917166788, 'label_smoothing_factor': 0.013176710880770154, 'num_train_epochs': 4, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 8, 'hidden_dropout_prob': 0.12498476100031271, 'attention_probs_dropout_prob': 0.13337070852537758, 'classifier_dropout': 0.013653702417085977}. Best is trial 0 with value: 0.8209848006399731.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 04:14, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.999700</td>\n",
       "      <td>1.977014</td>\n",
       "      <td>0.160534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.951500</td>\n",
       "      <td>1.929088</td>\n",
       "      <td>0.206919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.910100</td>\n",
       "      <td>1.871287</td>\n",
       "      <td>0.319481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.837900</td>\n",
       "      <td>1.821410</td>\n",
       "      <td>0.405667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.816600</td>\n",
       "      <td>1.788136</td>\n",
       "      <td>0.508254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.776500</td>\n",
       "      <td>1.776294</td>\n",
       "      <td>0.521516</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:15:42,170] Trial 4 finished with value: 0.5215158772081233 and parameters: {'learning_rate': 1.7630831492758887e-06, 'weight_decay': 0.02392619507540458, 'warmup_ratio': 0.06134211068449948, 'label_smoothing_factor': 0.08591468024278594, 'num_train_epochs': 6, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 16, 'hidden_dropout_prob': 0.08768726667708296, 'attention_probs_dropout_prob': 0.12506730203512761, 'classifier_dropout': 0.2293099576164588}. Best is trial 0 with value: 0.8209848006399731.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/66 00:19 < 02:00, 0.46 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.991300</td>\n",
       "      <td>2.016549</td>\n",
       "      <td>0.046032</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:16:05,807] Trial 5 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/110 00:40 < 02:57, 0.50 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.016100</td>\n",
       "      <td>1.922687</td>\n",
       "      <td>0.127201</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:16:49,989] Trial 6 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/44 00:12 < 00:13, 1.59 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.980400</td>\n",
       "      <td>1.987861</td>\n",
       "      <td>0.099609</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:17:05,156] Trial 7 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/88 00:12 < 00:41, 1.58 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.981100</td>\n",
       "      <td>2.026447</td>\n",
       "      <td>0.058382</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:17:20,583] Trial 8 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/44 01:24, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.975900</td>\n",
       "      <td>1.960337</td>\n",
       "      <td>0.092031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.836200</td>\n",
       "      <td>1.671815</td>\n",
       "      <td>0.491802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.530700</td>\n",
       "      <td>1.349127</td>\n",
       "      <td>0.694812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.248600</td>\n",
       "      <td>1.225850</td>\n",
       "      <td>0.678543</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:18:50,330] Trial 9 finished with value: 0.6785427040455503 and parameters: {'learning_rate': 1.5850185831538467e-05, 'weight_decay': 0.028537139725173837, 'warmup_ratio': 0.1120932196977931, 'label_smoothing_factor': 0.019547019012890486, 'num_train_epochs': 4, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 16, 'hidden_dropout_prob': 0.1288698617886836, 'attention_probs_dropout_prob': 0.13710951532739796, 'classifier_dropout': 0.19838594745760849}. Best is trial 0 with value: 0.8209848006399731.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/88 00:43 < 00:46, 0.96 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.931400</td>\n",
       "      <td>1.836673</td>\n",
       "      <td>0.188397</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:19:37,119] Trial 10 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/220 03:43, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.866600</td>\n",
       "      <td>1.533804</td>\n",
       "      <td>0.625352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.913800</td>\n",
       "      <td>0.830073</td>\n",
       "      <td>0.816641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.576400</td>\n",
       "      <td>0.844706</td>\n",
       "      <td>0.832778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.503300</td>\n",
       "      <td>0.848210</td>\n",
       "      <td>0.829249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.491200</td>\n",
       "      <td>0.865455</td>\n",
       "      <td>0.833175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:23:24,350] Trial 11 finished with value: 0.8331747515484594 and parameters: {'learning_rate': 1.0525941760282817e-05, 'weight_decay': 0.05915668758733891, 'warmup_ratio': 0.07468712461718222, 'label_smoothing_factor': 0.08870069043666225, 'num_train_epochs': 5, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 8, 'hidden_dropout_prob': 0.056651371447117016, 'attention_probs_dropout_prob': 0.050133066818597916, 'classifier_dropout': 0.07309078005487793}. Best is trial 11 with value: 0.8331747515484594.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/220 02:58 < 00:45, 0.98 it/s, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.872000</td>\n",
       "      <td>1.623926</td>\n",
       "      <td>0.517949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.033700</td>\n",
       "      <td>0.771411</td>\n",
       "      <td>0.800031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.541400</td>\n",
       "      <td>0.768134</td>\n",
       "      <td>0.804595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.452100</td>\n",
       "      <td>0.781000</td>\n",
       "      <td>0.814947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:26:25,563] Trial 12 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/220 00:43 < 03:01, 0.97 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.920900</td>\n",
       "      <td>1.729136</td>\n",
       "      <td>0.307777</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:27:12,007] Trial 13 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='132' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [132/132 02:13, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.909700</td>\n",
       "      <td>1.620380</td>\n",
       "      <td>0.543421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.128700</td>\n",
       "      <td>1.011144</td>\n",
       "      <td>0.802779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.851900</td>\n",
       "      <td>1.012896</td>\n",
       "      <td>0.815089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:29:29,955] Trial 14 finished with value: 0.815088859846073 and parameters: {'learning_rate': 1.2333260767233207e-05, 'weight_decay': 0.05971147353315741, 'warmup_ratio': 0.08282857906156649, 'label_smoothing_factor': 0.14919721722575507, 'num_train_epochs': 3, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 8, 'hidden_dropout_prob': 0.0856367033862454, 'attention_probs_dropout_prob': 0.2428774892183811, 'classifier_dropout': 0.06712973192350705}. Best is trial 11 with value: 0.8331747515484594.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/220 00:44 < 03:04, 0.95 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.978600</td>\n",
       "      <td>1.966773</td>\n",
       "      <td>0.136650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:30:16,989] Trial 15 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/176 00:43 < 02:17, 0.96 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.905900</td>\n",
       "      <td>1.702445</td>\n",
       "      <td>0.411704</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:31:03,632] Trial 16 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 88/132 01:28 < 00:45, 0.97 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.785500</td>\n",
       "      <td>1.284966</td>\n",
       "      <td>0.706652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.950600</td>\n",
       "      <td>0.685650</td>\n",
       "      <td>0.781972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:32:34,999] Trial 17 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 01:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.777800</td>\n",
       "      <td>1.294140</td>\n",
       "      <td>0.661748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.764400</td>\n",
       "      <td>0.800078</td>\n",
       "      <td>0.828767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.548400</td>\n",
       "      <td>0.836465</td>\n",
       "      <td>0.817098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.489800</td>\n",
       "      <td>0.845116</td>\n",
       "      <td>0.825780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.486400</td>\n",
       "      <td>0.832749</td>\n",
       "      <td>0.835433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:34:31,727] Trial 18 finished with value: 0.8354326838991185 and parameters: {'learning_rate': 2.4865048827022483e-05, 'weight_decay': 0.011925315394717216, 'warmup_ratio': 0.06931172912847788, 'label_smoothing_factor': 0.0914534859724227, 'num_train_epochs': 5, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 8, 'hidden_dropout_prob': 0.0504076419900225, 'attention_probs_dropout_prob': 0.0512470635035888, 'classifier_dropout': 0.09244814034175923}. Best is trial 18 with value: 0.8354326838991185.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 01:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.698600</td>\n",
       "      <td>1.130612</td>\n",
       "      <td>0.675718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.788200</td>\n",
       "      <td>0.919568</td>\n",
       "      <td>0.823679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.636600</td>\n",
       "      <td>0.947804</td>\n",
       "      <td>0.801677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.585500</td>\n",
       "      <td>0.936804</td>\n",
       "      <td>0.821660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.576100</td>\n",
       "      <td>0.951690</td>\n",
       "      <td>0.814666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:36:27,948] Trial 19 finished with value: 0.8146658095101057 and parameters: {'learning_rate': 2.4558057679007825e-05, 'weight_decay': 0.014724735449225649, 'warmup_ratio': 0.019432647621325745, 'label_smoothing_factor': 0.11804530779109057, 'num_train_epochs': 5, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 8, 'hidden_dropout_prob': 0.055798018713470565, 'attention_probs_dropout_prob': 0.05481277177789498, 'classifier_dropout': 0.09339576264675367}. Best is trial 18 with value: 0.8354326838991185.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='220' max='264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [220/264 02:02 < 00:24, 1.78 it/s, Epoch 5/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.664500</td>\n",
       "      <td>1.074368</td>\n",
       "      <td>0.730676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.706800</td>\n",
       "      <td>0.866641</td>\n",
       "      <td>0.817080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.548600</td>\n",
       "      <td>0.913892</td>\n",
       "      <td>0.832091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.489100</td>\n",
       "      <td>0.951102</td>\n",
       "      <td>0.803239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.492300</td>\n",
       "      <td>0.955921</td>\n",
       "      <td>0.829734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:38:32,827] Trial 20 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/110 00:21 < 01:34, 0.93 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.884800</td>\n",
       "      <td>1.771541</td>\n",
       "      <td>0.317924</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:38:57,325] Trial 21 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/88 00:21 < 01:11, 0.92 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.802000</td>\n",
       "      <td>1.579438</td>\n",
       "      <td>0.472448</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:39:21,947] Trial 22 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 00:59, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.952100</td>\n",
       "      <td>1.940085</td>\n",
       "      <td>0.140508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.580700</td>\n",
       "      <td>1.340479</td>\n",
       "      <td>0.668305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.001000</td>\n",
       "      <td>0.932347</td>\n",
       "      <td>0.750226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.710200</td>\n",
       "      <td>0.822801</td>\n",
       "      <td>0.801190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.636000</td>\n",
       "      <td>0.813139</td>\n",
       "      <td>0.796232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:40:25,581] Trial 23 finished with value: 0.7962321494930192 and parameters: {'learning_rate': 1.8888942363700907e-05, 'weight_decay': 0.01638264984513816, 'warmup_ratio': 0.08920462625641197, 'label_smoothing_factor': 0.09072620380289746, 'num_train_epochs': 5, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 8, 'hidden_dropout_prob': 0.07470934451655822, 'attention_probs_dropout_prob': 0.06967712189121002, 'classifier_dropout': 0.13451814414273106}. Best is trial 18 with value: 0.8354326838991185.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/132 00:42 < 01:30, 0.98 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.981900</td>\n",
       "      <td>1.950125</td>\n",
       "      <td>0.106803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:41:11,819] Trial 24 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/88 00:21 < 01:10, 0.94 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.843100</td>\n",
       "      <td>1.695370</td>\n",
       "      <td>0.418084</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:41:36,430] Trial 25 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 88/220 01:28 < 02:15, 0.97 it/s, Epoch 2/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.711800</td>\n",
       "      <td>1.152009</td>\n",
       "      <td>0.768198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.875800</td>\n",
       "      <td>0.881033</td>\n",
       "      <td>0.787309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:43:08,431] Trial 26 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/132 00:21 < 01:59, 0.92 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.972800</td>\n",
       "      <td>2.005272</td>\n",
       "      <td>0.036267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:43:33,222] Trial 27 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/110 00:12 < 00:55, 1.59 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.939000</td>\n",
       "      <td>1.872071</td>\n",
       "      <td>0.141061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:43:48,368] Trial 28 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='264' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/264 00:43 < 03:46, 0.97 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.937600</td>\n",
       "      <td>1.798123</td>\n",
       "      <td>0.345968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:44:34,897] Trial 29 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/88 00:40 < 02:12, 0.50 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.953800</td>\n",
       "      <td>1.918152</td>\n",
       "      <td>0.190717</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:45:19,476] Trial 30 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/132 00:41 < 03:45, 0.49 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.949600</td>\n",
       "      <td>1.800616</td>\n",
       "      <td>0.294939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:46:04,345] Trial 31 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/132 00:41 < 03:47, 0.48 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.975600</td>\n",
       "      <td>1.842811</td>\n",
       "      <td>0.276583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:46:49,607] Trial 32 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 02:06, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.794300</td>\n",
       "      <td>1.426105</td>\n",
       "      <td>0.722841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.128400</td>\n",
       "      <td>0.963708</td>\n",
       "      <td>0.791788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.833700</td>\n",
       "      <td>0.929473</td>\n",
       "      <td>0.792143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:48:59,901] Trial 33 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/88 00:40 < 02:12, 0.50 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.988400</td>\n",
       "      <td>1.896649</td>\n",
       "      <td>0.187199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:49:44,131] Trial 34 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 02:07, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.818400</td>\n",
       "      <td>1.546888</td>\n",
       "      <td>0.554486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.151500</td>\n",
       "      <td>0.944277</td>\n",
       "      <td>0.768770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.720700</td>\n",
       "      <td>0.875594</td>\n",
       "      <td>0.820985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.607400</td>\n",
       "      <td>0.895169</td>\n",
       "      <td>0.811692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.589900</td>\n",
       "      <td>0.881858</td>\n",
       "      <td>0.834749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.544300</td>\n",
       "      <td>0.885871</td>\n",
       "      <td>0.834749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:51:56,679] Trial 35 finished with value: 0.834748684899061 and parameters: {'learning_rate': 1.896284398882251e-05, 'weight_decay': 0.056111569909978204, 'warmup_ratio': 0.029687199808861767, 'label_smoothing_factor': 0.10407601390283198, 'num_train_epochs': 6, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 16, 'hidden_dropout_prob': 0.06343595620610712, 'attention_probs_dropout_prob': 0.1438100767788793, 'classifier_dropout': 0.12486183325035866}. Best is trial 18 with value: 0.8354326838991185.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 02:06, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.751800</td>\n",
       "      <td>1.422530</td>\n",
       "      <td>0.684908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.071700</td>\n",
       "      <td>0.986105</td>\n",
       "      <td>0.784430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.777500</td>\n",
       "      <td>1.006376</td>\n",
       "      <td>0.826158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.703500</td>\n",
       "      <td>1.005521</td>\n",
       "      <td>0.808137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.696300</td>\n",
       "      <td>0.971618</td>\n",
       "      <td>0.820985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.651000</td>\n",
       "      <td>0.974679</td>\n",
       "      <td>0.808137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:54:08,167] Trial 36 finished with value: 0.8081369948509909 and parameters: {'learning_rate': 2.172694071501705e-05, 'weight_decay': 0.05559337077072292, 'warmup_ratio': 0.00012133597909618626, 'label_smoothing_factor': 0.13747113096628732, 'num_train_epochs': 6, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 16, 'hidden_dropout_prob': 0.06255986507460509, 'attention_probs_dropout_prob': 0.14533451950147475, 'classifier_dropout': 0.1178924079636893}. Best is trial 18 with value: 0.8354326838991185.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/220 00:23 < 01:39, 1.77 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.802000</td>\n",
       "      <td>1.430314</td>\n",
       "      <td>0.466647</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:54:34,663] Trial 37 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/110 00:21 < 01:35, 0.93 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.789200</td>\n",
       "      <td>1.469013</td>\n",
       "      <td>0.579419</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:54:59,200] Trial 38 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 02:07, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.847200</td>\n",
       "      <td>1.598895</td>\n",
       "      <td>0.454439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.081900</td>\n",
       "      <td>0.894213</td>\n",
       "      <td>0.794291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.636600</td>\n",
       "      <td>0.911858</td>\n",
       "      <td>0.809196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.549900</td>\n",
       "      <td>0.904064</td>\n",
       "      <td>0.802252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.524200</td>\n",
       "      <td>0.870861</td>\n",
       "      <td>0.820985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.486100</td>\n",
       "      <td>0.867777</td>\n",
       "      <td>0.815089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:57:11,064] Trial 39 finished with value: 0.815088859846073 and parameters: {'learning_rate': 2.420319792312882e-05, 'weight_decay': 0.019468303079426118, 'warmup_ratio': 0.07341579402530032, 'label_smoothing_factor': 0.09477356066057999, 'num_train_epochs': 6, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 16, 'hidden_dropout_prob': 0.05030623642951107, 'attention_probs_dropout_prob': 0.11691401248388195, 'classifier_dropout': 0.10671531595442324}. Best is trial 18 with value: 0.8354326838991185.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/88 00:12 < 00:40, 1.62 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.937300</td>\n",
       "      <td>1.910665</td>\n",
       "      <td>0.167043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:57:25,827] Trial 40 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [66/66 02:06, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.907900</td>\n",
       "      <td>1.783634</td>\n",
       "      <td>0.317014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.449100</td>\n",
       "      <td>1.213470</td>\n",
       "      <td>0.739836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.933100</td>\n",
       "      <td>0.925492</td>\n",
       "      <td>0.790215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.704000</td>\n",
       "      <td>0.890966</td>\n",
       "      <td>0.817456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.652300</td>\n",
       "      <td>0.902014</td>\n",
       "      <td>0.820985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.604600</td>\n",
       "      <td>0.896380</td>\n",
       "      <td>0.820985</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 05:59:37,112] Trial 41 finished with value: 0.8209848006399731 and parameters: {'learning_rate': 1.531596098799703e-05, 'weight_decay': 0.056532252130244015, 'warmup_ratio': 0.031174935737121122, 'label_smoothing_factor': 0.10974300922028277, 'num_train_epochs': 6, 'per_device_train_batch_size': 4, 'gradient_accumulation_steps': 16, 'hidden_dropout_prob': 0.06754135466577883, 'attention_probs_dropout_prob': 0.0632577062234707, 'classifier_dropout': 0.1827286571779857}. Best is trial 18 with value: 0.8354326838991185.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/66 00:19 < 02:00, 0.46 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.986900</td>\n",
       "      <td>1.971781</td>\n",
       "      <td>0.094973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:00:00,633] Trial 42 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/66 01:02 < 01:06, 0.49 it/s, Epoch 3/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.894900</td>\n",
       "      <td>1.738574</td>\n",
       "      <td>0.328146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.409600</td>\n",
       "      <td>1.146951</td>\n",
       "      <td>0.761319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.871100</td>\n",
       "      <td>0.908223</td>\n",
       "      <td>0.788015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:01:07,306] Trial 43 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='55' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [55/55 01:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.863400</td>\n",
       "      <td>1.611837</td>\n",
       "      <td>0.445177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.181300</td>\n",
       "      <td>0.977635</td>\n",
       "      <td>0.778355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.760800</td>\n",
       "      <td>0.946232</td>\n",
       "      <td>0.815089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.672700</td>\n",
       "      <td>0.952278</td>\n",
       "      <td>0.815089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.662500</td>\n",
       "      <td>0.954379</td>\n",
       "      <td>0.815089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:02:56,438] Trial 44 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/66 00:19 < 01:58, 0.46 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.947300</td>\n",
       "      <td>1.876144</td>\n",
       "      <td>0.247792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:03:19,838] Trial 45 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/110 00:21 < 01:34, 0.93 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.929900</td>\n",
       "      <td>1.793089</td>\n",
       "      <td>0.309563</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:03:44,420] Trial 46 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [36/36 01:08, Epoch 6/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.966600</td>\n",
       "      <td>2.007146</td>\n",
       "      <td>0.068578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.896300</td>\n",
       "      <td>1.934540</td>\n",
       "      <td>0.139160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.781800</td>\n",
       "      <td>1.784057</td>\n",
       "      <td>0.379539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.621400</td>\n",
       "      <td>1.662479</td>\n",
       "      <td>0.540478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.538500</td>\n",
       "      <td>1.583526</td>\n",
       "      <td>0.616605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.486800</td>\n",
       "      <td>1.554621</td>\n",
       "      <td>0.635901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:04:57,719] Trial 47 finished with value: 0.6359013657150303 and parameters: {'learning_rate': 8.227870112801962e-06, 'weight_decay': 0.025230542519839562, 'warmup_ratio': 0.0280498868284621, 'label_smoothing_factor': 0.13846975412765666, 'num_train_epochs': 6, 'per_device_train_batch_size': 8, 'gradient_accumulation_steps': 16, 'hidden_dropout_prob': 0.056096263398251224, 'attention_probs_dropout_prob': 0.15462960589943905, 'classifier_dropout': 0.12679005710459276}. Best is trial 18 with value: 0.8354326838991185.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/110 00:21 < 01:34, 0.93 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.807500</td>\n",
       "      <td>1.463327</td>\n",
       "      <td>0.633519</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:05:22,123] Trial 48 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/88 00:21 < 01:10, 0.93 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.959800</td>\n",
       "      <td>1.885342</td>\n",
       "      <td>0.268615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:05:46,583] Trial 49 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/176 00:43 < 02:16, 0.97 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.860200</td>\n",
       "      <td>1.506311</td>\n",
       "      <td>0.442845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:06:32,963] Trial 50 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/132 00:40 < 03:43, 0.49 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.976600</td>\n",
       "      <td>1.836132</td>\n",
       "      <td>0.246552</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:07:17,538] Trial 51 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/132 00:39 < 03:39, 0.50 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.945600</td>\n",
       "      <td>1.686924</td>\n",
       "      <td>0.402941</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:08:01,289] Trial 52 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/132 00:39 < 03:39, 0.50 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.935400</td>\n",
       "      <td>1.608078</td>\n",
       "      <td>0.462575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:08:45,506] Trial 53 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/132 00:40 < 03:43, 0.49 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.849400</td>\n",
       "      <td>1.554721</td>\n",
       "      <td>0.506730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:09:29,845] Trial 54 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/88 00:23 < 00:25, 1.76 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.980200</td>\n",
       "      <td>1.966092</td>\n",
       "      <td>0.080522</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:09:56,196] Trial 55 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/220 00:43 < 03:01, 0.97 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.926300</td>\n",
       "      <td>1.821377</td>\n",
       "      <td>0.218133</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:10:42,380] Trial 56 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='55' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/55 00:11 < 00:54, 0.81 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.941600</td>\n",
       "      <td>1.941788</td>\n",
       "      <td>0.112443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:10:56,495] Trial 57 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='11' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [11/66 00:19 < 02:01, 0.45 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.925100</td>\n",
       "      <td>1.868790</td>\n",
       "      <td>0.187782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:11:20,094] Trial 58 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='220' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/220 00:43 < 03:02, 0.97 it/s, Epoch 1/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.996900</td>\n",
       "      <td>1.967530</td>\n",
       "      <td>0.077596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:12:06,441] Trial 59 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [22/88 00:21 < 01:11, 0.93 it/s, Epoch 1/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.908900</td>\n",
       "      <td>1.790751</td>\n",
       "      <td>0.332891</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:12:31,046] Trial 60 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/88 00:43 < 00:45, 0.97 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.901400</td>\n",
       "      <td>1.623631</td>\n",
       "      <td>0.471211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:13:17,286] Trial 61 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/132 00:43 < 01:30, 0.97 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.878400</td>\n",
       "      <td>1.608832</td>\n",
       "      <td>0.449785</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:14:03,674] Trial 62 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/132 00:43 < 01:30, 0.97 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.880100</td>\n",
       "      <td>1.582803</td>\n",
       "      <td>0.639640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:14:49,911] Trial 63 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/132 00:43 < 01:30, 0.97 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.531900</td>\n",
       "      <td>0.993276</td>\n",
       "      <td>0.788789</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:15:36,054] Trial 64 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 44/132 00:42 < 01:29, 0.98 it/s, Epoch 1/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.868800</td>\n",
       "      <td>1.510570</td>\n",
       "      <td>0.609587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:16:21,977] Trial 65 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='22' max='132' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 22/132 00:39 < 03:38, 0.50 it/s, Epoch 1/6]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.943500</td>\n",
       "      <td>1.769583</td>\n",
       "      <td>0.372195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:17:06,107] Trial 66 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.386300</td>\n",
       "      <td>0.955135</td>\n",
       "      <td>0.814528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.766200</td>\n",
       "      <td>0.945129</td>\n",
       "      <td>0.832187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:18:45,094] Trial 67 finished with value: 0.8321866173369933 and parameters: {'learning_rate': 1.6246407639816003e-05, 'weight_decay': 0.020519323582265266, 'warmup_ratio': 0.08158813558843554, 'label_smoothing_factor': 0.12173444825278314, 'num_train_epochs': 2, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 4, 'hidden_dropout_prob': 0.179766877861056, 'attention_probs_dropout_prob': 0.08935193503255112, 'classifier_dropout': 0.09568225057853891}. Best is trial 18 with value: 0.8354326838991185.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.314800</td>\n",
       "      <td>0.955995</td>\n",
       "      <td>0.822982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.763800</td>\n",
       "      <td>0.932956</td>\n",
       "      <td>0.843625</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:20:24,606] Trial 68 finished with value: 0.8436253421075726 and parameters: {'learning_rate': 2.2498363132635124e-05, 'weight_decay': 0.008723835430920937, 'warmup_ratio': 0.11135886805699885, 'label_smoothing_factor': 0.12193024528952655, 'num_train_epochs': 2, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 4, 'hidden_dropout_prob': 0.1830835962885386, 'attention_probs_dropout_prob': 0.08959631581036148, 'classifier_dropout': 0.12426882226343645}. Best is trial 68 with value: 0.8436253421075726.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='44' max='88' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [44/88 00:23 < 00:24, 1.77 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.808400</td>\n",
       "      <td>1.305775</td>\n",
       "      <td>0.528469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:20:50,772] Trial 69 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.330700</td>\n",
       "      <td>0.962913</td>\n",
       "      <td>0.811962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.789900</td>\n",
       "      <td>0.920054</td>\n",
       "      <td>0.850848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:22:29,565] Trial 70 finished with value: 0.850848004626539 and parameters: {'learning_rate': 2.2663062908229715e-05, 'weight_decay': 0.009621609515299934, 'warmup_ratio': 0.09497451665661313, 'label_smoothing_factor': 0.1319158205445586, 'num_train_epochs': 2, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 4, 'hidden_dropout_prob': 0.1750046684707643, 'attention_probs_dropout_prob': 0.09193672404027178, 'classifier_dropout': 0.13750807676388485}. Best is trial 70 with value: 0.850848004626539.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.335500</td>\n",
       "      <td>0.989269</td>\n",
       "      <td>0.837399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.803600</td>\n",
       "      <td>0.952839</td>\n",
       "      <td>0.850848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:24:09,039] Trial 71 finished with value: 0.850848004626539 and parameters: {'learning_rate': 2.3068843508297868e-05, 'weight_decay': 0.013883206473110813, 'warmup_ratio': 0.10822844988425628, 'label_smoothing_factor': 0.13371889132662507, 'num_train_epochs': 2, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 4, 'hidden_dropout_prob': 0.18116435897000208, 'attention_probs_dropout_prob': 0.09173042817739417, 'classifier_dropout': 0.13859106177039468}. Best is trial 70 with value: 0.850848004626539.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.322600</td>\n",
       "      <td>0.968271</td>\n",
       "      <td>0.814832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.791100</td>\n",
       "      <td>0.957631</td>\n",
       "      <td>0.834318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:25:46,751] Trial 72 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 88/176 00:47 < 00:48, 1.81 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.395500</td>\n",
       "      <td>0.994856</td>\n",
       "      <td>0.808820</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:26:36,661] Trial 73 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.569300</td>\n",
       "      <td>0.996832</td>\n",
       "      <td>0.838574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.904300</td>\n",
       "      <td>0.963968</td>\n",
       "      <td>0.864152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:28:15,137] Trial 74 finished with value: 0.8641519508958911 and parameters: {'learning_rate': 2.3166819422109686e-05, 'weight_decay': 0.016327131338801114, 'warmup_ratio': 0.1031008761394571, 'label_smoothing_factor': 0.13210781093467075, 'num_train_epochs': 2, 'per_device_train_batch_size': 2, 'gradient_accumulation_steps': 4, 'hidden_dropout_prob': 0.2040551540427023, 'attention_probs_dropout_prob': 0.10332603594550857, 'classifier_dropout': 0.1423794900309442}. Best is trial 74 with value: 0.8641519508958911.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.329600</td>\n",
       "      <td>0.990015</td>\n",
       "      <td>0.814832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.788800</td>\n",
       "      <td>0.962497</td>\n",
       "      <td>0.823589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:29:53,204] Trial 75 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 88/176 00:47 < 00:48, 1.82 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.981000</td>\n",
       "      <td>1.697155</td>\n",
       "      <td>0.563535</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:30:43,119] Trial 76 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:36, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.388800</td>\n",
       "      <td>1.053104</td>\n",
       "      <td>0.818088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.834000</td>\n",
       "      <td>1.015289</td>\n",
       "      <td>0.835750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:32:22,324] Trial 77 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='176' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [176/176 01:35, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.316800</td>\n",
       "      <td>0.965654</td>\n",
       "      <td>0.814832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.940072</td>\n",
       "      <td>0.840604</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:34:00,771] Trial 78 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='88' max='176' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 88/176 00:47 < 00:48, 1.80 it/s, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.633300</td>\n",
       "      <td>0.974378</td>\n",
       "      <td>0.786083</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-24 06:34:51,319] Trial 79 pruned. \n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:20, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at ai-forever/ruRoberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 01:19, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     entertainment       0.86      0.67      0.75         9\n",
      "         geography       0.73      1.00      0.84         8\n",
      "            health       0.92      1.00      0.96        11\n",
      "          politics       1.00      1.00      1.00        14\n",
      "science/technology       1.00      0.92      0.96        25\n",
      "            sports       0.92      1.00      0.96        12\n",
      "            travel       0.89      0.85      0.87        20\n",
      "\n",
      "          accuracy                           0.92        99\n",
      "         macro avg       0.90      0.92      0.91        99\n",
      "      weighted avg       0.93      0.92      0.92        99\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "     entertainment       0.88      0.74      0.80        19\n",
      "         geography       0.80      0.94      0.86        17\n",
      "            health       0.91      0.95      0.93        22\n",
      "          politics       1.00      0.97      0.98        30\n",
      "science/technology       0.94      0.96      0.95        51\n",
      "            sports       0.88      0.88      0.88        25\n",
      "            travel       0.87      0.85      0.86        40\n",
      "\n",
      "          accuracy                           0.91       204\n",
      "         macro avg       0.90      0.90      0.90       204\n",
      "      weighted avg       0.91      0.91      0.91       204\n",
      "\n"
     ]
    }
   ],
   "source": [
    "BEST_MODEL_HP = {}\n",
    "\n",
    "def model_init(trial=None):\n",
    "    if trial is None:\n",
    "        hid_d = BEST_MODEL_HP.get(\"hidden_dropout_prob\", 0.1)\n",
    "        attn_d = BEST_MODEL_HP.get(\"attention_probs_dropout_prob\", 0.1)\n",
    "        cls_d = BEST_MODEL_HP.get(\"classifier_dropout\", 0.1)\n",
    "    else:\n",
    "        hid_d = trial.suggest_float(\"hidden_dropout_prob\", 0.05, 0.25)\n",
    "        attn_d = trial.suggest_float(\"attention_probs_dropout_prob\", 0.05, 0.25)\n",
    "        cls_d = trial.suggest_float(\"classifier_dropout\", 0.0, 0.3)\n",
    "\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=n_categories,\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    config.hidden_dropout_prob = hid_d\n",
    "    config.attention_probs_dropout_prob = attn_d\n",
    "    config.classifier_dropout = cls_d\n",
    "\n",
    "    return AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=config)\n",
    "\n",
    "class_weights = None\n",
    "sampler = None\n",
    "\n",
    "def set_balancing(train_ds):\n",
    "    global class_weights, sampler\n",
    "    y = np.array(train_ds[\"label\"])\n",
    "    classes = np.unique(y)\n",
    "    cw = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y)\n",
    "    class_weights = torch.tensor(cw, dtype=torch.float)\n",
    "    SAMPLE_ALPHA = 0.5\n",
    "    sample_w = (class_weights[y] ** SAMPLE_ALPHA)\n",
    "    sampler = WeightedRandomSampler(weights=sample_w, num_samples=len(sample_w), replacement=True)\n",
    "class BalancedTrainer(Trainer):\n",
    "    def get_train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.args.per_device_train_batch_size,\n",
    "            sampler=sampler,\n",
    "            collate_fn=self.data_collator,\n",
    "            drop_last=False,\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.get(\"labels\", inputs.get(\"label\"))\n",
    "        model_inputs = {k: v for k, v in inputs.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n",
    "        outputs = model(**model_inputs)\n",
    "        w = class_weights.to(outputs.logits.device)\n",
    "        loss = F.cross_entropy(\n",
    "            outputs.logits,\n",
    "            labels,\n",
    "            weight=w,\n",
    "            label_smoothing=float(getattr(self.args, \"label_smoothing_factor\", 0.0)),\n",
    "        )\n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def hp_space(trial):\n",
    "    return {\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 2.5e-5, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 0.0, 0.06),\n",
    "        \"warmup_ratio\": trial.suggest_float(\"warmup_ratio\", 0.0, 0.12),\n",
    "        \"label_smoothing_factor\": trial.suggest_float(\"label_smoothing_factor\", 0.0, 0.15),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 2, 6),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [2, 4, 8]),\n",
    "        \"gradient_accumulation_steps\": trial.suggest_categorical(\"gradient_accumulation_steps\", [4, 8, 16]),\n",
    "    }\n",
    "\n",
    "search_args = TrainingArguments(\n",
    "    output_dir=\"hp_search\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    seed=42,\n",
    "    data_seed=42,\n",
    ")\n",
    "\n",
    "set_seed(42)\n",
    "set_balancing(labeled_train_set)\n",
    "\n",
    "trainer = BalancedTrainer(\n",
    "    model_init=model_init,\n",
    "    args=search_args,\n",
    "    train_dataset=labeled_train_set,\n",
    "    eval_dataset=labeled_validation_set,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "best = trainer.hyperparameter_search(\n",
    "    backend=\"optuna\",\n",
    "    direction=\"maximize\",\n",
    "    n_trials=80,\n",
    "    hp_space=hp_space,\n",
    "    compute_objective=lambda m: m[\"eval_f1\"],\n",
    ")\n",
    "\n",
    "best_hp = dict(best.hyperparameters)\n",
    "\n",
    "drop_keys = [\"hidden_dropout_prob\", \"attention_probs_dropout_prob\", \"classifier_dropout\"]\n",
    "BEST_MODEL_HP = {k: best_hp.pop(k) for k in drop_keys if k in best_hp}\n",
    "\n",
    "best_hp[\"per_device_eval_batch_size\"] = best_hp[\"per_device_train_batch_size\"]\n",
    "\n",
    "full_train = concatenate_datasets([labeled_train_set, labeled_validation_set])\n",
    "set_balancing(full_train)\n",
    "\n",
    "def predict_logits(model, ds):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "    dl = DataLoader(ds, batch_size=32, collate_fn=data_collator, drop_last=False)\n",
    "    outs = []\n",
    "    for batch in dl:\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k in [\"input_ids\", \"attention_mask\", \"token_type_ids\"]}\n",
    "        with torch.no_grad():\n",
    "            outs.append(model(**batch).logits.detach().cpu())\n",
    "    return torch.cat(outs, dim=0)\n",
    "\n",
    "\n",
    "seeds = [13, 21, 42, 77, 101, 123, 202]\n",
    "logits_val = None\n",
    "logits_test = None\n",
    "\n",
    "for s in seeds:\n",
    "    set_seed(s)\n",
    "    args_s = TrainingArguments(\n",
    "        output_dir=f\"final_full_{s}\",\n",
    "        eval_strategy=\"no\",\n",
    "        save_strategy=\"no\",\n",
    "        logging_strategy=\"no\",\n",
    "        report_to=\"none\",\n",
    "        fp16=True,\n",
    "        lr_scheduler_type=\"cosine\",\n",
    "        seed=s,\n",
    "        data_seed=s,\n",
    "        **best_hp,\n",
    "    )\n",
    "    tr = BalancedTrainer(\n",
    "        model_init=model_init,\n",
    "        args=args_s,\n",
    "        train_dataset=full_train,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    tr.train()\n",
    "    m = tr.model.cuda()\n",
    "    lv = predict_logits(m, labeled_validation_set)\n",
    "    lt = predict_logits(m, labeled_test_set)\n",
    "    logits_val = lv if logits_val is None else logits_val + lv\n",
    "    logits_test = lt if logits_test is None else logits_test + lt\n",
    "    del m, tr\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "logits_val = logits_val / len(seeds)\n",
    "logits_test = logits_test / len(seeds)\n",
    "\n",
    "pred_val = logits_val.argmax(dim=1).numpy().tolist()\n",
    "pred_test = logits_test.argmax(dim=1).numpy().tolist()\n",
    "\n",
    "y_pred_val = [id2label[i] for i in pred_val]\n",
    "y_true_val = [id2label[i] for i in list(labeled_validation_set[\"label\"])]\n",
    "print(classification_report(y_true=y_true_val, y_pred=y_pred_val))\n",
    "\n",
    "y_pred_test = [id2label[i] for i in pred_test]\n",
    "y_true_test = [id2label[i] for i in list(labeled_test_set[\"label\"])]\n",
    "print(classification_report(y_true=y_true_test, y_pred=y_pred_test))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
